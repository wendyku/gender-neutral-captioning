{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['food', 'baseball', 'umbrella', 'kitchen', 'tie', 'snowboard', 'skateboard', 'table', 'cell', 'motorcycle'])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagged lists of Activity/ object images from kayburns' Github Repo \"Women Snowboard\"\n",
    "# https://github.com/kayburns/women-snowboard\n",
    "\n",
    "activity_list_paths = glob.glob(\"./data/list/intersection_*\")\n",
    "activity_image_ids = dict()\n",
    "for path in activity_list_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        p = re.compile(r'(?<=_)(.*)(?=_)')\n",
    "        activity = p.findall(path)[0]\n",
    "        im_ids= f.read().split('\\n')\n",
    "        im_ids = [int(i) for i in im_ids if i != '']\n",
    "        activity_image_ids[activity] = im_ids\n",
    "\n",
    "activity_image_ids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['female', 'male', 'neutral'])\n"
     ]
    }
   ],
   "source": [
    "# Gender word lists from sueqian6's Github Repo \"Reducing Gender Bias in Word-level Language Models\"\n",
    "# https://github.com/sueqian6/ACL2019-Reducing-Gender-Bias-in-Word-Level-Language-Models-Using-A-Gender-Equalizing-Loss-Function\n",
    "\n",
    "gender_list_paths = glob.glob(\"./data/list/*_word_file.txt\")\n",
    "gender_list_paths.append('./data/list/neutral_occupations.txt')\n",
    "gender_nouns_lookup = dict()\n",
    "for path in gender_list_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        if path == './data/list/neutral_occupations.txt':\n",
    "            gender = 'neutral'\n",
    "        else:\n",
    "            p = re.compile(r'(?<=list/)(.*)(?=_word)')\n",
    "            gender = p.findall(path)[0]\n",
    "        nouns = f.read().split('\\n')\n",
    "        nouns = [n for n in nouns if n != '']\n",
    "        gender_nouns_lookup[gender] = nouns\n",
    "\n",
    "print(gender_nouns_lookup.keys())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some inappropriate in the word lists, we are doing some hand crafting to avoid adding bias.\n",
    "# Remove non-human words\n",
    "words = ['cow', 'cows', 'hen', 'hens']\n",
    "for word in words:\n",
    "    gender_nouns_lookup['female'].remove(word)\n",
    "words = ['bull', 'bulls', 'lion', 'lions', 'governor']\n",
    "for word in words:\n",
    "    gender_nouns_lookup['male'].remove(word)\n",
    "    \n",
    "# Add gender-neutral words\n",
    "words = ['surfer', 'child', 'kid', 'kids', 'children', 'passenger', 'passengers',\\\n",
    "         'governor', 'someone', 'pedestrian', 'pedestrians']\n",
    "for word in words:\n",
    "    gender_nouns_lookup['neutral'].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agreement_score(anno_gender):\n",
    "    error = 0\n",
    "    score_cal_dict ={\n",
    "        'male':1, 'female':-1, 'neutral':0\n",
    "    }\n",
    "    gt = max(set(anno_gender), key = anno_gender.count)\n",
    "    \n",
    "    for ind, p in enumerate(anno_gender):\n",
    "        for other_p in [x for i,x in enumerate(anno_gender) if i != ind]:\n",
    "            error += np.abs(score_cal_dict[p] - score_cal_dict[other_p])\n",
    "            \n",
    "    score = (24 - error) / 24\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caption 0 processed, out of 414113 captions\n",
      "No. of qualified images processed: 0\n",
      "\n",
      "Caption 100000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 6452\n",
      "\n",
      "Caption 200000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 13359\n",
      "\n",
      "Caption 300000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 24080\n",
      "\n",
      "Caption 400000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 34712\n",
      "\n",
      "Caption 0 processed, out of 202654 captions\n",
      "No. of qualified images processed: 35500\n",
      "\n",
      "Caption 100000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 42016\n",
      "\n",
      "Caption 200000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 52292\n"
     ]
    }
   ],
   "source": [
    "annotations_path = './data/annotations/'\n",
    "'''\n",
    "captions_dict (dict)- key: image_id, value: list of captions\n",
    "\n",
    "im_gender_summary (dict of dict)- key: image_id, value: dict()\n",
    "keys in dict: pred_gt- predicted ground truth label of the gender noun\n",
    "              per_gt- % of annotations (out of 5 total) that agreed with the GT\n",
    "              agreement_score- agreement score calculated using distance between 5 predictions, with 1 being the best\n",
    "                               male = 1, female = -1, neutral = 0\n",
    "                               e.g.0 annotations indicate [f, f, f, f, f], agreement_score = 1.00\n",
    "                               e.g.1 annotations indicate [m, m, f, f, f], agreement_score = 0.00\n",
    "                               e.g.2 annotation indicate [n, n, f, f, f], agreement_score = 0.50                                                        \n",
    "              anno_gender- list of gender sentiment, e.g. ['male', 'female', 'neutral', 'female', 'female']\n",
    "              anno_nouns- list of nouns used to describe human\n",
    "              clean_gender- binary variable indicating if all notations used the same gender/ gender-neutral noun \n",
    "              clean_noun- binary variable indicating if all notations used the identical noun\n",
    "\n",
    "not_human_im_ids(list)- list of image ids of images with >1 captions that do not mention humans.\n",
    "Since the COCO dataset does not label whether human (or other objects) is the major subject \n",
    "matter of the image. This list helps us isolate images with human figures as the focus.\n",
    "'''\n",
    "captions_dict = dict()\n",
    "im_gender_summary = dict()\n",
    "not_human_im_ids = list() \n",
    "\n",
    "for datatype in ['train', 'val']:\n",
    "    print(f\"Evaluating ground truth labels in {datatype} set')\n",
    "        with open(f'{annotations_path}/captions_{datatype}2014.json') as f:\n",
    "        captions_json = json.load(f)\n",
    "\n",
    "        for i in range(len(captions_json['annotations'])):\n",
    "            image_id = captions_json['annotations'][i]['image_id']\n",
    "            caption = captions_json['annotations'][i]['caption']\n",
    "            tokens = nltk.word_tokenize(caption)\n",
    "            c_female = 0 # count of gender nouns and gender-neutral nouns\n",
    "            c_male = 0\n",
    "            c_neutral = 0\n",
    "            noun = []\n",
    "\n",
    "            # Evaluate annotator's noun used to describe humans\n",
    "            for t in tokens:\n",
    "                t = t.lower()\n",
    "                if t in gender_nouns_lookup['female']:\n",
    "                    c_female += 1\n",
    "                    noun.append(t)\n",
    "                elif t in gender_nouns_lookup['male']:\n",
    "                    c_male += 1\n",
    "                    noun.append(t)\n",
    "                elif t in gender_nouns_lookup['neutral']:\n",
    "                    c_neutral += 1\n",
    "                    noun.append(t)\n",
    "\n",
    "            # Only include image for training if more than one caption of the image mention human\n",
    "            # Conflicting gender mentions are also dropped, e.g. \"a boy and a girl are on a beach\"\n",
    "            if c_female + c_male + c_neutral == 1:\n",
    "                # Assign gender sentiment to the caption\n",
    "                if c_female > 0:\n",
    "                    gender = 'female'\n",
    "                elif c_male > 0:\n",
    "                    gender = 'male'\n",
    "                else:\n",
    "                    gender = 'neutral'\n",
    "\n",
    "                # Populate captions dict and image gender summary dict\n",
    "                if image_id in captions_dict:\n",
    "                    captions_dict[image_id] += [caption]\n",
    "                    im_gender_summary[image_id]['anno_gender'].append(gender)\n",
    "                    im_gender_summary[image_id]['anno_noun'].append(noun[0])\n",
    "                else:\n",
    "                    captions_dict[image_id] = [caption]\n",
    "                    im_gender_summary[image_id] = dict()\n",
    "                    im_gender_summary[image_id]['anno_gender'] = [gender]\n",
    "                    im_gender_summary[image_id]['anno_noun'] = [noun[0]]\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print()\n",
    "                print(f\"Caption {i} processed, out of {len(captions_json['annotations'])} captions\")\n",
    "                print(f\"No. of qualified images processed: {len(im_gender_summary)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_id in im_gender_summary:\n",
    "    error = 0\n",
    "    \n",
    "    # Delete images where <3 annotators mentioned the human figure\n",
    "    # Because it is impossible to estimate the ground truth using only 1 or 2 captions \n",
    "    if len(im_gender_summary[image_id]['anno_gender']) < 3:\n",
    "        not_human_im_ids.append(image_id)\n",
    "    \n",
    "    else:\n",
    "        pred = im_gender_summary[image_id]['anno_gender']\n",
    "\n",
    "        # Evaluate groundtruth guesses and agreement scores\n",
    "        gt = max(set(pred), key = pred.count)\n",
    "\n",
    "        # Populate dictionary\n",
    "        im_gender_summary[image_id]['pred_gt'] = gt\n",
    "        im_gender_summary[image_id]['per_gt'] = sum([1 for p in pred if p == gt]) / len(pred)\n",
    "        im_gender_summary[image_id]['agreement_score'] = agreement_score(pred)\n",
    "        if len(set(pred)) == 1:\n",
    "            im_gender_summary[image_id]['clean_gender'] = 1\n",
    "        else:\n",
    "            im_gender_summary[image_id]['clean_gender'] = 0\n",
    "        if len(set(im_gender_summary[image_id]['anno_noun'])) == 1:\n",
    "            im_gender_summary[image_id]['clean_noun'] = 1\n",
    "        else:\n",
    "            im_gender_summary[image_id]['clean_noun'] = 0\n",
    "            \n",
    "for image_id in not_human_im_ids:\n",
    "    try:\n",
    "        del captions_dict[image_id]\n",
    "        del im_gender_summary[image_id]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anno_gender': ['female', 'female', 'female', 'female'], 'anno_noun': ['women', 'woman', 'woman', 'women'], 'pred_gt': 'female', 'per_gt': 1.0, 'agreement_score': 1.0, 'clean_gender': 1, 'clean_noun': 0}\n",
      "{'anno_gender': ['female', 'female', 'female', 'female', 'female'], 'anno_noun': ['woman', 'woman', 'woman', 'woman', 'woman'], 'pred_gt': 'female', 'per_gt': 1.0, 'agreement_score': 1.0, 'clean_gender': 1, 'clean_noun': 1}\n",
      "{'anno_gender': ['male', 'male', 'male', 'male'], 'anno_noun': ['man', 'man', 'man', 'man'], 'pred_gt': 'male', 'per_gt': 1.0, 'agreement_score': 1.0, 'clean_gender': 1, 'clean_noun': 1}\n",
      "{'anno_gender': ['male', 'male', 'male'], 'anno_noun': ['man', 'man', 'man'], 'pred_gt': 'male', 'per_gt': 1.0, 'agreement_score': 1.0, 'clean_gender': 1, 'clean_noun': 1}\n",
      "{'anno_gender': ['male', 'male', 'male', 'male', 'male'], 'anno_noun': ['man', 'man', 'man', 'man', 'man'], 'pred_gt': 'male', 'per_gt': 1.0, 'agreement_score': 1.0, 'clean_gender': 1, 'clean_noun': 1}\n"
     ]
    }
   ],
   "source": [
    "for key in list(im_gender_summary.keys())[:5]:\n",
    "    print(im_gender_summary[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save list of image_ids of qualified images\n",
    "with open('./data/list/qualified_image_ids.csv', \"w\") as outfile:\n",
    "    for image_id in im_gender_summary.keys():\n",
    "        outfile.write(str(image_id))\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(im_gender_summary, captions_dict, training_size, mode = 'random'):\n",
    "    assert mode in ['random','balanced_mode','balanced_clean', 'balanced_gender_only', \\\n",
    "                    'balanced_clean_noun', 'clean_noun', 'activity_balanced', 'activity_balanced_clean']\n",
    "    assert isinstance(training_size, int)\n",
    "    '''\n",
    "    8 different modes of generating data\n",
    "    - random: randomized selection of qualified images\n",
    "    - balanced_mode: balanced ratio between male, female and neutral\n",
    "    - balanced_clean: balanced ratio between male, female and neutral,\n",
    "                      only use images when all captions agree on using the same gender\n",
    "    - balanced_gender_only: same as balanced_mode, but without neutral captions\n",
    "    - balanced_clean_noun: balanced ratio between male, female and neutral, only use images when all captions\n",
    "                           agree on using the same noun\n",
    "    - clean_noun: only use images when all captions agree on the same noun\n",
    "    - activity_balanced: from activity tagged image sets, choose same ratio of male, female, neutral image\n",
    "    - activity_balanced_clean: similar to activity_balanced, but all captions must agree on the same gender\n",
    "    \n",
    "    Note that it is possible that output size may be smaller than training_size,\n",
    "    especially for activity_balanced and activity_balanced_clean. As for certain activities, the sample size of\n",
    "    clean data might be limited for some classes, e.g. women wearing tie.\n",
    "    '''\n",
    "    \n",
    "    random.seed(123)\n",
    "    training_captions_dict = dict()\n",
    "    \n",
    "    if mode == 'random':\n",
    "        training_captions_dict = dict(random.sample(captions_dict.items(), training_size))\n",
    "        \n",
    "    elif mode == 'balanced_mode':\n",
    "        i = 0\n",
    "        male_count = 0\n",
    "        female_count = 0\n",
    "        neutral_count = 0\n",
    "        for image_id in im_gender_summary.keys():\n",
    "            if i < training_size:\n",
    "                if im_gender_summary[image_id]['pred_gt'] == 'male' and (male_count < training_size / 3):\n",
    "                    training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                    male_count += 1\n",
    "                    i += 1\n",
    "                elif im_gender_summary[image_id]['pred_gt'] == 'female' and (female_count < training_size / 3):\n",
    "                    training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                    female_count += 1\n",
    "                    i += 1\n",
    "                elif im_gender_summary[image_id]['pred_gt'] == 'neutral'and (neutral_count < training_size / 3):\n",
    "                    training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                    neutral_count += 1\n",
    "                    i += 1\n",
    "                    \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"captions of {i} images are added\")\n",
    "    \n",
    "    elif mode == 'balanced_clean':\n",
    "        i = 0\n",
    "        male_count = 0\n",
    "        female_count = 0\n",
    "        neutral_count = 0\n",
    "        for image_id in im_gender_summary.keys():\n",
    "            if i < training_size:\n",
    "                if im_gender_summary[image_id]['clean_gender'] == 1:\n",
    "                    if im_gender_summary[image_id]['pred_gt'] == 'male' and (male_count < training_size / 3):\n",
    "                        training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                        male_count += 1\n",
    "                        i += 1\n",
    "                    elif im_gender_summary[image_id]['pred_gt'] == 'female' and (female_count < training_size / 3):\n",
    "                        training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                        female_count += 1\n",
    "                        i += 1\n",
    "                    elif im_gender_summary[image_id]['pred_gt'] == 'neutral'and (neutral_count < training_size / 3):\n",
    "                        training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                        neutral_count += 1\n",
    "                        i += 1\n",
    "                    \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"captions of {i} images are added\")\n",
    "    \n",
    "    elif mode == 'balanced_clean_noun':\n",
    "        i = 0\n",
    "        male_count = 0\n",
    "        female_count = 0\n",
    "        neutral_count = 0\n",
    "        for image_id in im_gender_summary.keys():\n",
    "            if i < training_size:\n",
    "                if im_gender_summary[image_id]['clean_noun'] == 1:\n",
    "                    if im_gender_summary[image_id]['pred_gt'] == 'male' and (male_count < training_size / 3):\n",
    "                        training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                        male_count += 1\n",
    "                        i += 1\n",
    "                    elif im_gender_summary[image_id]['pred_gt'] == 'female' and (female_count < training_size / 3):\n",
    "                        training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                        female_count += 1\n",
    "                        i += 1\n",
    "                    elif im_gender_summary[image_id]['pred_gt'] == 'neutral'and (neutral_count < training_size / 3):\n",
    "                        training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                        neutral_count += 1\n",
    "                        i += 1\n",
    "                    \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"captions of {i} images are added\") \n",
    "                    \n",
    "    elif mode == 'clean_noun':\n",
    "        i = 0\n",
    "        for image_id in im_gender_summary.keys():\n",
    "            if i < training_size:\n",
    "                if im_gender_summary[image_id]['clean_noun'] == 1:\n",
    "                    training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                    i += 1\n",
    "                    \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"captions of {i} images are added\")   \n",
    "    \n",
    "    elif mode == 'balanced_gender_only':\n",
    "        i = 0\n",
    "        male_count = 0\n",
    "        female_count = 0\n",
    "        for image_id in im_gender_summary.keys():\n",
    "            if i < training_size:\n",
    "                if im_gender_summary[image_id]['pred_gt'] == 'male' and (male_count < training_size / 2):\n",
    "                    training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                    male_count += 1\n",
    "                    i += 1\n",
    "                elif im_gender_summary[image_id]['pred_gt'] == 'female' and (female_count < training_size / 2):\n",
    "                    training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                    female_count += 1\n",
    "                    i += 1\n",
    "                    \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"captions of {i} images are added\")\n",
    "    \n",
    "    elif mode == 'activity_balanced':\n",
    "        activity_training_size = training_size / len(activity_image_ids.keys())\n",
    "        i = 0\n",
    "        for activity in activity_image_ids.keys():\n",
    "            image_ids = activity_image_ids[activity]\n",
    "            j = 0\n",
    "            male_count = 0\n",
    "            female_count = 0\n",
    "            neutral_count = 0\n",
    "            for image_id in image_ids:\n",
    "                if j < activity_training_size:\n",
    "                    if image_id in im_gender_summary:\n",
    "                        if im_gender_summary[image_id]['pred_gt'] == 'male' and (male_count < activity_training_size / 3):\n",
    "                            training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                            male_count += 1\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "                        elif im_gender_summary[image_id]['pred_gt'] == 'female' and (female_count < activity_training_size / 3):\n",
    "                            training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                            female_count += 1\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "                        elif im_gender_summary[image_id]['pred_gt'] == 'neutral'and (neutral_count < activity_training_size / 3):\n",
    "                            training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                            neutral_count += 1\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "\n",
    "                    if i > 0 and i % 100 == 0:\n",
    "                        print(f\"captions of {i} images are added\")\n",
    "    \n",
    "    elif mode == 'activity_balanced_clean':\n",
    "        activity_training_size = training_size / len(activity_image_ids.keys())\n",
    "        i = 0\n",
    "        for activity in activity_image_ids.keys():\n",
    "            image_ids = activity_image_ids[activity]\n",
    "            j = 0\n",
    "            male_count = 0\n",
    "            female_count = 0\n",
    "            neutral_count = 0\n",
    "            for image_id in image_ids:\n",
    "                if j < activity_training_size:\n",
    "                    if image_id in im_gender_summary and im_gender_summary[image_id]['clean_noun'] == 1:\n",
    "                        if im_gender_summary[image_id]['pred_gt'] == 'male' and (male_count < activity_training_size / 3):\n",
    "                            training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                            male_count += 1\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "                        elif im_gender_summary[image_id]['pred_gt'] == 'female' and (female_count < activity_training_size / 3):\n",
    "                            training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                            female_count += 1\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "                        elif im_gender_summary[image_id]['pred_gt'] == 'neutral'and (neutral_count < activity_training_size / 3):\n",
    "                            training_captions_dict[image_id] = captions_dict[image_id]\n",
    "                            neutral_count += 1\n",
    "                            i += 1\n",
    "                            j += 1\n",
    "\n",
    "                        if i > 0 and i % 100 == 0:\n",
    "                            print(f\"captions of {i} images are added\")\n",
    "    \n",
    "    return training_captions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions of 100 images are added\n",
      "captions of 200 images are added\n",
      "captions of 300 images are added\n",
      "captions of 400 images are added\n",
      "captions of 500 images are added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = get_training_data(im_gender_summary, captions_dict, training_size = 1000, mode = 'activity_balanced_clean')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
