{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from dataset import MyDataset\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from data_utils import get_gender_nouns\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from data_utils import get_test_indices\n",
    "import math\n",
    "\n",
    "# Frequency of printing batch loss while training/validating. \n",
    "print_interval = 1000\n",
    "\n",
    "'''\n",
    "Default data loader, train, validate functions\n",
    "'''\n",
    "def load_data(image_ids, image_folder_path, mode):\n",
    "    # Initiate instance of MyDataset class\n",
    "    num_workers = 0\n",
    "    dataset = MyDataset(image_ids, image_folder_path, mode = mode)\n",
    "    if mode == 'train' or mode == 'val':\n",
    "        indices = dataset.get_indices()\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader = data.DataLoader(dataset=dataset, num_workers=num_workers,\\\n",
    "                    batch_sampler=\\\n",
    "                    data.sampler.BatchSampler(sampler=initial_sampler,\\\n",
    "                    batch_size=dataset.batch_size,drop_last=False))\n",
    "    else: # if test, initial sampler is not necessary\n",
    "        data_loader = data.DataLoader(dataset=dataset, num_workers=num_workers,\\\n",
    "                 batch_size=dataset.batch_size, shuffle = True)  \n",
    "    return data_loader\n",
    "\n",
    "def train(train_loader, encoder, decoder, criterion, optimizer, vocab_size,\n",
    "          epoch, total_step, start_step=1, start_loss=0.0):\n",
    "    \"\"\"Train the model for one epoch using the provided parameters. Save \n",
    "    checkpoints every 100 steps. Return the epoch's average train loss.\"\"\"\n",
    "\n",
    "    # Switch to train mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    # Keep track of train loss\n",
    "    total_loss = start_loss\n",
    "\n",
    "    # Start time for every 100 steps\n",
    "    start_train_time = time.time()\n",
    "\n",
    "    for i_step in range(start_step, total_step + 1):\n",
    "        # Randomly sample a caption length, and sample indices with that length\n",
    "        indices = train_loader.dataset.get_indices()\n",
    "        # Create a batch sampler to retrieve a batch with the sampled indices\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        train_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch\n",
    "        for batch in train_loader:\n",
    "            images, captions = batch[0], batch[1]\n",
    "            break \n",
    "        # Move to GPU if CUDA is available\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            captions = captions.cuda()\n",
    "        # Pass the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        # Zero the gradients. Since the backward() function accumulates \n",
    "        # gradients, and we donâ€™t want to mix up gradients between minibatches,\n",
    "        # we have to zero them out at the start of a new minibatch\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass to calculate the weight gradients\n",
    "        loss.backward()\n",
    "        # Update the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Get training statistics\n",
    "        stats = \"Epoch %d, Train step [%d/%d], %ds, Loss: %.4f, Perplexity: %5.4f\" \\\n",
    "                % (epoch, i_step, total_step, time.time() - start_train_time,\n",
    "                   loss.item(), np.exp(loss.item()))\n",
    "        # Print training statistics (on same line)\n",
    "        print(\"\\r\" + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training stats (on different line), reset time and save checkpoint\n",
    "        if i_step % print_interval == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "            filename = os.path.join(\"./models\", \"train-model-{}{}.pkl\".format(epoch, i_step))\n",
    "            save_checkpoint(filename, encoder, decoder, optimizer, total_loss, epoch, i_step)\n",
    "            start_train_time = time.time()\n",
    "            \n",
    "    return total_loss / total_step\n",
    "            \n",
    "def validate(val_loader, encoder, decoder, criterion, vocab, epoch, \n",
    "             total_step, start_step=1, start_loss=0.0, start_bleu=0.0):\n",
    "    \"\"\"Validate the model for one epoch using the provided parameters. \n",
    "    Return the epoch's average validation loss and Bleu-4 score.\"\"\"\n",
    "\n",
    "    # Switch to validation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Initialize smoothing function\n",
    "    smoothing = SmoothingFunction()\n",
    "\n",
    "    # Keep track of validation loss and Bleu-4 score\n",
    "    total_loss = start_loss\n",
    "    total_bleu_4 = start_bleu\n",
    "\n",
    "    # Start time for every 100 steps\n",
    "    start_val_time = time.time()\n",
    "\n",
    "    # Disable gradient calculation because we are in inference mode\n",
    "    with torch.no_grad():\n",
    "        for i_step in range(start_step, total_step + 1):\n",
    "            # Randomly sample a caption length, and sample indices with that length\n",
    "            indices = val_loader.dataset.get_indices()\n",
    "            # Create a batch sampler to retrieve a batch with the sampled indices\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            val_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain the batch\n",
    "            for batch in val_loader:\n",
    "                images, captions = batch[0], batch[1]\n",
    "                break \n",
    "\n",
    "            # Move to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                captions = captions.cuda()\n",
    "            \n",
    "            # Pass the inputs through the CNN-RNN model\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "\n",
    "            # Calculate the total Bleu-4 score for the batch\n",
    "            batch_bleu_4 = 0.0\n",
    "            # Iterate over outputs. Note: outputs[i] is a caption in the batch\n",
    "            # outputs[i, j, k] contains the model's predicted score i.e. how \n",
    "            # likely the j-th token in the i-th caption in the batch is the \n",
    "            # k-th token in the vocabulary.\n",
    "            for i in range(len(outputs)):\n",
    "                predicted_ids = []\n",
    "                for scores in outputs[i]:\n",
    "                    # Find the index of the token that has the max score\n",
    "                    predicted_ids.append(scores.argmax().item())\n",
    "                # Convert word ids to actual words\n",
    "                predicted_word_list = word_list(predicted_ids, vocab)\n",
    "                caption_word_list = word_list(captions[i].cpu().numpy(), vocab)\n",
    "                # Calculate Bleu-4 score and append it to the batch_bleu_4 list\n",
    "                batch_bleu_4 += sentence_bleu([caption_word_list], \n",
    "                                               predicted_word_list, \n",
    "                                               smoothing_function=smoothing.method1)\n",
    "            total_bleu_4 += batch_bleu_4 / len(outputs)\n",
    "\n",
    "            # Calculate the batch loss\n",
    "            loss = criterion(outputs.view(-1, len(vocab)), captions.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get validation statistics\n",
    "            stats = \"Epoch %d, Val step [%d/%d], %ds, Loss: %.4f, Perplexity: %5.4f, Bleu-4: %.4f\" \\\n",
    "                    % (epoch, i_step, total_step, time.time() - start_val_time,\n",
    "                       loss.item(), np.exp(loss.item()), batch_bleu_4 / len(outputs))\n",
    "\n",
    "            # Print validation statistics (on same line)\n",
    "            print(\"\\r\" + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Print validation statistics (on different line) and reset time\n",
    "            if i_step % print_interval == 0:\n",
    "                print(\"\\r\" + stats)\n",
    "                filename = os.path.join(\"./models\", \"val-model-{}{}.pkl\".format(epoch, i_step))\n",
    "                save_val_checkpoint(filename, encoder, decoder, total_loss, total_bleu_4, epoch, i_step)\n",
    "                start_val_time = time.time()\n",
    "                \n",
    "        return total_loss / total_step, total_bleu_4 / total_step\n",
    "\n",
    "'''\n",
    "Gender-neutral train and validate functions\n",
    "'''\n",
    "def gender_neutral_loss_setup(train_loader, vocab_size):\n",
    "    gender_nouns=get_gender_nouns()\n",
    "    male_tags=gender_nouns['male']\n",
    "    female_tags=gender_nouns['female']\n",
    "    neutral_tags=gender_nouns['neutral']\n",
    "    vocab = train_loader.dataset.vocab\n",
    "    \n",
    "    ##Construct fonehot, monehot and nonehot vectors from the vocab\n",
    "    vocab_words=word_list_vocab(list(range(vocab_size)),vocab)\n",
    "\n",
    "    global fonehot, monehot, nonehot\n",
    "    fonehot = torch.zeros(len(vocab))\n",
    "    monehot =torch.zeros(len(vocab))\n",
    "    nonehot=torch.zeros(len(vocab))\n",
    "    for i,word in enumerate(vocab_words):\n",
    "        ##female one hot vector (all female associated words are tagged 1 and the rest are 0's)\n",
    "        if any(word==fem_word for fem_word in female_tags):\n",
    "            fonehot[i]=1\n",
    "        ##male one hot vector\n",
    "        if any(word==male_word for male_word in male_tags):\n",
    "            monehot[i]=1\n",
    "        ##neutral one hot vector\n",
    "        if any(word==neut_word for neut_word in neutral_tags):\n",
    "            nonehot[i]=1\n",
    "\n",
    "def word_list_vocab(word_idx_list, vocab):\n",
    "    \"\"\"Take a list of word ids and a vocabulary from a dataset as inputs\n",
    "    and return the corresponding words as a list.\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    for i in range(len(word_idx_list)):\n",
    "        vocab_id = word_idx_list[i]\n",
    "        word = vocab.idx2word[vocab_id]\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def cross_entr(outputs,targets):\n",
    "    loss = -1 / outputs.shape[1] * (torch.matmul(outputs,torch.log(targets.t())) + torch.matmul( 1 - outputs, torch.log(1 - targets.t())))\n",
    "\n",
    "def loss_function(outputs,captions,female_ops,male_ops,neutral_opsm, vocab_size):\n",
    "    ##Cross entropy loss\n",
    "    ce_loss=criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "    ##Log loss that penalizes biased gender ratio (i.e., male/female or female/male)\n",
    "    b_loss = torch.mean(torch.abs(torch.log((torch.exp(female_ops) + 0.00001) / (torch.exp(male_ops) + 0.00001))))   \n",
    "    \n",
    "    return ce_loss + b_loss\n",
    "\n",
    "def train_gender_neutral(train_loader, encoder, decoder, criterion, optimizer, vocab_size,\n",
    "          epoch, total_step, start_step=1, start_loss=0.0):\n",
    "    \"\"\"Train the model for one epoch using the provided parameters. Save \n",
    "    checkpoints every 100 steps. Return the epoch's average train loss.\"\"\"\n",
    "\n",
    "    # Switch to train mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    # Keep track of train loss\n",
    "    total_loss = start_loss\n",
    "\n",
    "    # Start time for every 100 steps\n",
    "    start_train_time = time.time()\n",
    "\n",
    "    for i_step in range(start_step, total_step + 1):\n",
    "        # Randomly sample a caption length, and sample indices with that length\n",
    "        indices = train_loader.dataset.get_indices()\n",
    "        # Create a batch sampler to retrieve a batch with the sampled indices\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        train_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch\n",
    "        for batch in train_loader:\n",
    "            images, captions = batch[0], batch[1]\n",
    "            break \n",
    "        # Move to GPU if CUDA is available\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            captions = captions.cuda()\n",
    "        # Pass the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        female_ops=0\n",
    "        male_ops=0\n",
    "        neutral_ops=0\n",
    "        \n",
    "        for i in range(len(outputs)):\n",
    "                for scores in outputs[i]:\n",
    "                    #0 out all non-gendered words in scores and find the sum of the scores for all gendered words\n",
    "                    female_ops+=torch.matmul(fonehot,scores.t())\n",
    "                    male_ops+=torch.matmul(monehot,scores.t())\n",
    "                    neutral_ops+=torch.matmul(nonehot,scores.t())\n",
    "                female_ops/=len(outputs[i])\n",
    "                male_ops/=len(outputs[i])\n",
    "                neutral_ops/=len(outputs[i])\n",
    "\n",
    "        # Calculate the batch loss \n",
    "        loss=loss_function(outputs,captions,female_ops,male_ops,neutral_ops, vocab_size)\n",
    "        \n",
    "        # Zero the gradients. Since the backward() function accumulates gradients, and we donâ€™t want to mix up gradients between minibatches,\n",
    "        # we have to zero them out at the start of a new minibatch\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass to calculate the weight gradients\n",
    "        loss.backward()\n",
    "        # Update the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Get training statistics\n",
    "        stats = \"Epoch %d, Train step [%d/%d], %ds, Loss: %.4f, Perplexity: %5.4f\" \\\n",
    "                % (epoch, i_step, total_step, time.time() - start_train_time,\n",
    "                   loss.item(), np.exp(loss.item()))\n",
    "        # Print training statistics (on same line)\n",
    "        print(\"\\r\" + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print training stats (on different line), reset time and save checkpoint\n",
    "        if i_step % print_interval == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "            filename = os.path.join(\"./models\", \"train-model-{}{}.pkl\".format(epoch, i_step))\n",
    "            save_checkpoint(filename, encoder, decoder, optimizer, total_loss, epoch, i_step)\n",
    "            start_train_time = time.time()\n",
    "            \n",
    "    return total_loss / total_step\n",
    "            \n",
    "def validate_gender_neutral(val_loader, encoder, decoder, criterion, vocab, vocab_size, epoch, \n",
    "             total_step, start_step=1, start_loss=0.0, start_bleu=0.0):\n",
    "    \"\"\"Validate the model for one epoch using the provided parameters. \n",
    "    Return the epoch's average validation loss and Bleu-4 score.\"\"\"\n",
    "\n",
    "    # Switch to validation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Initialize smoothing function\n",
    "    smoothing = SmoothingFunction()\n",
    "\n",
    "    # Keep track of validation loss and Bleu-4 score\n",
    "    total_loss = start_loss\n",
    "    total_bleu_4 = start_bleu\n",
    "\n",
    "    # Start time for every 100 steps\n",
    "    start_val_time = time.time()\n",
    "\n",
    "    # Disable gradient calculation because we are in inference mode\n",
    "    with torch.no_grad():\n",
    "        for i_step in range(start_step, total_step + 1):\n",
    "            # Randomly sample a caption length, and sample indices with that length\n",
    "            indices = val_loader.dataset.get_indices()\n",
    "            # Create a batch sampler to retrieve a batch with the sampled indices\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            val_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain the batch\n",
    "            for batch in val_loader:\n",
    "                images, captions = batch[0], batch[1]\n",
    "                break \n",
    "\n",
    "            # Move to GPU if CUDA is available\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                captions = captions.cuda()\n",
    "            \n",
    "            # Pass the inputs through the CNN-RNN model\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "            \n",
    "            female_ops=0\n",
    "            male_ops=0\n",
    "            neutral_ops=0\n",
    "\n",
    "            # Calculate the total Bleu-4 score for the batch\n",
    "            batch_bleu_4 = 0.0\n",
    "            # Iterate over outputs. Note: outputs[i] is a caption in the batch\n",
    "            # outputs[i, j, k] contains the model's predicted score i.e. how \n",
    "            # likely the j-th token in the i-th caption in the batch is the \n",
    "            # k-th token in the vocabulary.\n",
    "            for i in range(len(outputs)):\n",
    "                predicted_ids = []\n",
    "                for scores in outputs[i]:\n",
    "                    # Find the index of the token that has the max score\n",
    "                    predicted_ids.append(scores.argmax().item())\n",
    "                    #0 out all non-gendered words in scores and find the sum of the scores for all gendered words\n",
    "                    female_ops+=torch.matmul(fonehot,scores.t())\n",
    "                    male_ops+=torch.matmul(monehot,scores.t())\n",
    "                    neutral_ops+=torch.matmul(nonehot,scores.t())\n",
    "                female_ops/=len(outputs[i])\n",
    "                male_ops/=len(outputs[i])\n",
    "                neutral_ops/=len(outputs[i])\n",
    "                    \n",
    "                # Convert word ids to actual words\n",
    "                predicted_word_list = word_list(predicted_ids, vocab)\n",
    "                caption_word_list = word_list(captions[i].numpy(), vocab)\n",
    "                # Calculate Bleu-4 score and append it to the batch_bleu_4 list\n",
    "                batch_bleu_4 += sentence_bleu([caption_word_list], \n",
    "                                               predicted_word_list, \n",
    "                                               smoothing_function=smoothing.method1)\n",
    "            total_bleu_4 += batch_bleu_4 / len(outputs)\n",
    "\n",
    "            # Calculate the batch loss\n",
    "            loss=loss_function(outputs,captions,female_ops,male_ops,neutral_ops, vocab_size)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get validation statistics\n",
    "            stats = \"Epoch %d, Val step [%d/%d], %ds, Loss: %.4f, Perplexity: %5.4f, Bleu-4: %.4f\" \\\n",
    "                    % (epoch, i_step, total_step, time.time() - start_val_time,\n",
    "                       loss.item(), np.exp(loss.item()), batch_bleu_4 / len(outputs))\n",
    "\n",
    "            # Print validation statistics (on same line)\n",
    "            print(\"\\r\" + stats, end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Print validation statistics (on different line) and reset time\n",
    "            if i_step % print_interval == 0:\n",
    "                print(\"\\r\" + stats)\n",
    "                filename = os.path.join(\"./models\", \"val-model-{}{}.pkl\".format(epoch, i_step))\n",
    "                save_val_checkpoint(filename, encoder, decoder, total_loss, total_bleu_4, epoch, i_step)\n",
    "                start_val_time = time.time()\n",
    "                \n",
    "        return total_loss / total_step, total_bleu_4 / total_step\n",
    "\n",
    "'''\n",
    "Umbrella training model function using all above functions\n",
    "'''\n",
    "def loader_setup(train_image_ids, val_image_ids, image_folder_path):\n",
    "    train_loader = load_data(train_image_ids, image_folder_path, mode = 'train')\n",
    "    val_loader = load_data(val_image_ids, image_folder_path, mode = 'val')\n",
    "    print('\\n\\nLoaders successfully set up . . .')\n",
    "\n",
    "    # Sample a subset of captions with a randomized length\n",
    "    indices = train_loader.dataset.get_indices()\n",
    "\n",
    "    # Create and assign batch sampler to retrieve a batch with the sampled indices\n",
    "    new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "    train_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "    # Obtain the batch\n",
    "    for batch in train_loader:\n",
    "        images, captions = batch[0], batch [1]\n",
    "    \n",
    "    print('\\n\\nChecking shape of sample batch . . .')\n",
    "    print('images.shape:', images.shape)\n",
    "    print('captions.shape:', captions.shape)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(train_image_ids, val_image_ids, image_folder_path, batch_size, embed_size, hidden_size, num_epochs, dataset_mode, loss_mode, mode = 'reg'):\n",
    "    assert mode in ['reg','gender_neutral']\n",
    "    #reg: regular training loss function\n",
    "    #gender_neural: alternative loss function that penalizes gender bias\n",
    "    # Set up\n",
    "    train_loader, val_loader = loader_setup(train_image_ids, val_image_ids, image_folder_path)\n",
    "    global vocab\n",
    "    vocab = train_loader.dataset.vocab\n",
    "    vocab_size = len(train_loader.dataset.vocab)\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"\\n\\nInitialize model . . .\")\n",
    "    # Initialize CNN and RNN\n",
    "    global encoder, decoder, criterion, optimizer\n",
    "    encoder = EncoderCNN(embed_size)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "    # Use GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()  \n",
    "        \n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "    # Specify the learnable parameters of the model\n",
    "    params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(params=params, lr=0.001)\n",
    "\n",
    "    # Calculate total number of training steps per epoch\n",
    "    print(\"\\n\\nCalculate total number of steps per epoch . . .\")\n",
    "    total_train_step = math.ceil(len(train_loader.dataset.captions_len) / train_loader.batch_sampler.batch_size)\n",
    "    print (\"Number of training steps:\", total_train_step)\n",
    "    total_val_step = math.ceil(len(val_loader.dataset.captions_len) / val_loader.batch_sampler.batch_size)\n",
    "    print (\"Number of training steps:\", total_val_step)\n",
    "\n",
    "    # Training\n",
    "    print(\"\\n\\nTraining model . . .\")\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_bleus = []\n",
    "    best_val_bleu = float(\"-INF\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if mode == 'reg':\n",
    "            train_loss = train(train_loader, encoder, decoder, criterion, optimizer, vocab_size, epoch, total_train_step)\n",
    "            val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion, train_loader.dataset.vocab, epoch, total_val_step)\n",
    "        else: # use gender_neutral train function\n",
    "            gender_neutral_loss_setup(train_loader, vocab_size)\n",
    "            train_loss = train_gender_neutral(train_loader, encoder, decoder, criterion, optimizer, vocab_size, epoch, total_train_step)\n",
    "            val_loss, val_bleu = validate_gender_neutral(val_loader, encoder, decoder, criterion, vocab, vocab_size, epoch, total_val_step)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_bleus.append(val_bleu)\n",
    "        if val_bleu > best_val_bleu:\n",
    "            print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "                format(best_val_bleu, val_bleu))\n",
    "            best_val_bleu = val_bleu\n",
    "            filename = f\"./testmodels/{dataset_mode}_{loss_mode}_best-model.pkl\"\n",
    "            save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                    val_bleu, val_bleus, epoch)\n",
    "        else:\n",
    "            print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "        # Save the entire model anyway, regardless of being the best model so far or not\n",
    "        filename = f\"./testmodels/{dataset_mode}_{loss_mode}model-{epoch}.pkl\"\n",
    "        save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                val_bleu, val_bleus, epoch)\n",
    "        print (\"Epoch [%d/%d] took %ds\" % (epoch, num_epochs, time.time() - start_time))\n",
    "        if epoch > 5:\n",
    "            # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "            if early_stopping(val_bleus, 3):\n",
    "                break\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_training_indices, train_test_split\n",
    "from model_utils import save_checkpoint, save_val_checkpoint, save_epoch, early_stopping, word_list, clean_sentence, get_prediction, predict_any_image\n",
    "import pickle\n",
    "\n",
    "sample_size = 5\n",
    "test_size = 0.3\n",
    "image_folder_path = './data/images/'\n",
    "\n",
    "dataset_modes = ['random','balanced_mode','balanced_clean', 'balanced_gender_only','balanced_clean_noun', 'clean_noun', 'activity_balanced', 'activity_balanced_clean']\n",
    "loss_modes = ['reg', 'gender_neutral']\n",
    "\n",
    "for dataset_mode in dataset_modes:\n",
    "    for loss_mode in loss_modes:\n",
    "        training_image_ids, training_captions_dict = get_training_indices(sample_size = sample_size,\\\n",
    "                                                                          mode = dataset_mode)\n",
    "        train_image_ids, val_image_ids, gender_train, gender_val = train_test_split(training_image_ids, test_size = test_size)\n",
    "\n",
    "        batch_size = 32\n",
    "        embed_size = 256\n",
    "        hidden_size = 512\n",
    "        num_epochs = 10\n",
    "\n",
    "        train_model(train_image_ids, val_image_ids, image_folder_path, batch_size, embed_size, \\\n",
    "                hidden_size, num_epochs, dataset_mode, loss_mode, mode = loss_mode)\n",
    "        # export vocab too\n",
    "        with open(f'./testmodels/{dataset_mode}_{lossmode}.pkl', 'wb') as f:\n",
    "            pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
