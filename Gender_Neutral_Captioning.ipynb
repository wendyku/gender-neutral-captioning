{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Neutral Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preparing Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_activity_list, get_gender_nouns, get_qualified_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_image_ids saved  as ~/obj/activity_image_ids.pkl\n",
      "gender_nouns_lookup saved  as ~/obj/gender_nouns_lookup.pkl\n",
      "Loading gender_nouns_lookup from ~/obj/gender_nouns_lookup.pkl\n",
      "\n",
      "Evaluating ground truth labels in train set\n",
      "Caption 100000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 4504\n",
      "Caption 200000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 9333\n",
      "Caption 400000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 24380\n",
      "\n",
      "Evaluating ground truth labels in val set\n",
      "Caption 0 processed, out of 202654 captions\n",
      "No. of qualified images processed: 24941\n",
      "Caption 100000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 31457\n",
      "Caption 200000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 41733\n",
      "saved in ./data/list/qualified_image_ids.csv\n",
      "captions_dict saved  as ~/obj/captions_dict.pkl\n",
      "im_gender_summary saved  as ~/obj/im_gender_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "annotations_path = './data/annotations/'\n",
    "get_activity_list(save_file = True)\n",
    "get_gender_nouns(save_file = True)\n",
    "get_qualified_dataset(annotations_path, save_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select method to generate training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our motivation of the project is to counter the bias in the dataset. As ground truth labels are not availabie from the original COCO dataset, we are experimenting with different methods of balancing the dataset. In the **get_training_indices** function in data_utils.py, there are 8 different modes of generating data.\n",
    "\n",
    "    - random: randomized selection of qualified images\n",
    "    - balanced_mode: balanced ratio between male, female and neutral\n",
    "    - balanced_clean: balanced ratio between male, female and neutral, only use images when all captions agree on using the same gender\n",
    "    - balanced_gender_only: same as balanced_mode, but without neutral captions\n",
    "    - balanced_clean_noun: balanced ratio between male, female and neutral, only use images when all captions agree on using the same noun\n",
    "    - clean_noun: only use images when all captions agree on the same noun\n",
    "    - activity_balanced: from activity tagged image sets, choose same ratio of male, female, neutral image\n",
    "    - activity_balanced_clean: similar to activity_balanced, but all captions must agree on the same gender\n",
    "    \n",
    "Note that it is possible that output size may be smaller than training_size, especially for activity_balanced and activity_balanced_clean. As for certain activities, the sample size of clean data might be limited for some classes, e.g. women wearing tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading activity_image_ids from ~/obj/activity_image_ids.pkl\n",
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_training_indices, train_test_split\n",
    "\n",
    "sample_size = 10\n",
    "test_size = 0.3\n",
    "training_image_ids, training_captions_dict = get_training_indices(sample_size = sample_size, mode = \"balanced_clean\")\n",
    "train_image_ids, val_image_ids, gender_train, gender_val = train_test_split(training_image_ids, test_size = test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Tokenize captions: (0, 23)\n",
      "vocab saved  as ~/obj/vocab.pkl\n",
      "Vocabulary successfully created\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading vocab from ~/obj/vocab.pkl\n",
      "Vocabulary successfully loaded\n"
     ]
    }
   ],
   "source": [
    "from model_utils import load_data\n",
    "\n",
    "image_folder_path = './data/images/'\n",
    "\n",
    "train_loader = load_data(train_image_ids, image_folder_path, mode = 'train')\n",
    "val_loader = load_data(val_image_ids, image_folder_path, mode = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "# Sample a subset of captions with a randomized length\n",
    "indices = train_loader.dataset.get_indices()\n",
    "\n",
    "# Create and assign batch sampler to retrieve a batch with the sampled indices\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "train_loader.batch_sampler.sampler = new_sampler\n",
    "    \n",
    "# Load one batch\n",
    "# images, captions = next(iter(data_loader))\n",
    "\n",
    "# Obtain the batch\n",
    "for batch in train_loader:\n",
    "    images, captions = batch[0], batch [1]\n",
    "    \n",
    "print('images.shape:', images.shape)\n",
    "print('captions.shape:', captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps: 3\n",
      "Number of training steps: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 10\n",
    "vocab_size = len(train_loader.dataset.vocab)\n",
    "\n",
    "# Initialize CNN and RNN\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()  \n",
    "    \n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda()\\\n",
    "if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)\n",
    "\n",
    "# Calculate total number of training steps per epoch\n",
    "total_train_step = math.ceil(len(train_loader.dataset.captions_len) / train_loader.batch_sampler.batch_size)\n",
    "print (\"Number of training steps:\", total_train_step)\n",
    "total_val_step = math.ceil(len(val_loader.dataset.captions_len) / val_loader.batch_sampler.batch_size)\n",
    "print (\"Number of training steps:\", total_val_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val step [2/2], 0s, Loss: 1.8816, Perplexity: 6.5639, Bleu-4: 0.3375Validation Bleu-4 improved from -inf to 0.3501, saving model to best-model.pkl\n",
      "Epoch [1/10] took 3s\n",
      "Epoch 2, Val step [2/2], 0s, Loss: 1.6607, Perplexity: 5.2630, Bleu-4: 0.4903Validation Bleu-4 improved from 0.3501 to 0.3937, saving model to best-model.pkl\n",
      "Epoch [2/10] took 2s\n",
      "Epoch 3, Val step [2/2], 0s, Loss: 1.5851, Perplexity: 4.8800, Bleu-4: 0.1242Validation Bleu-4 did not improve, saving model to model-3.pkl\n",
      "Epoch [3/10] took 2s\n",
      "Epoch 4, Val step [2/2], 0s, Loss: 1.4941, Perplexity: 4.4551, Bleu-4: 0.4523Validation Bleu-4 did not improve, saving model to model-4.pkl\n",
      "Epoch [4/10] took 1s\n",
      "Epoch 5, Val step [2/2], 0s, Loss: 1.6074, Perplexity: 4.9896, Bleu-4: 0.2006Validation Bleu-4 did not improve, saving model to model-5.pkl\n",
      "Epoch [5/10] took 1s\n",
      "Epoch 6, Val step [2/2], 0s, Loss: 1.6262, Perplexity: 5.0846, Bleu-4: 0.5100Validation Bleu-4 improved from 0.3937 to 0.4038, saving model to best-model.pkl\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from model_utils import train, validate, save_epoch, early_stopping\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_bleus = []\n",
    "best_val_bleu = float(\"-INF\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                       vocab_size, epoch, total_train_step)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "                                  train_loader.dataset.vocab, epoch, total_val_step)\n",
    "    val_losses.append(val_loss)\n",
    "    val_bleus.append(val_bleu)\n",
    "    if val_bleu > best_val_bleu:\n",
    "        print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "               format(best_val_bleu, val_bleu))\n",
    "        best_val_bleu = val_bleu\n",
    "        filename = os.path.join(\"./models\", \"best-model.pkl\")\n",
    "        save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                   val_bleu, val_bleus, epoch)\n",
    "    else:\n",
    "        print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "    # Save the entire model anyway, regardless of being the best model so far or not\n",
    "    filename = os.path.join(\"./models\", \"model-{}.pkl\".format(epoch))\n",
    "    save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "               val_bleu, val_bleus, epoch)\n",
    "    print (\"Epoch [%d/%d] took %ds\" % (epoch, num_epochs, time.time() - start_time))\n",
    "    if epoch > 5:\n",
    "        # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "        if early_stopping(val_bleus, 3):\n",
    "            break\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Predict on test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model weights from XXX to ./model/ of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# Load the best model\n",
    "checkpoint = torch.load(os.path.join('./models', 'best-model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_test_indices\n",
    "sample_size = 5\n",
    "test_image_ids = get_test_indices(training_image_ids, sample_size, mode = 'balanced_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{174028: ['two girls sitting on a bench eating lunch',\n",
       "  'A girl demonstrating the act of shaving sitting on a bench.',\n",
       "  'two females sitting on a bench one is being the arms for the other.'],\n",
       " 385626: ['Woman riding a bicycle down an empty street.',\n",
       "  'A woman in green is riding a bike.',\n",
       "  'a woman wearing a bright green sweater riding a bicycle',\n",
       "  'A woman on a bicycle is going down the small town street.',\n",
       "  'A woman bikes down a one way street.'],\n",
       " 9469: ['A teenage boy is in a field looking at a wire.',\n",
       "  'The boy is inspecting the wire for a project.',\n",
       "  'A boy building something with wires and poles.'],\n",
       " 348595: ['A man in a parade has red and white face and umbrella.',\n",
       "  'Soccer fan is showing his support by dressing up. ',\n",
       "  'A man with a painted red and white face is standing in the street.'],\n",
       " 25174: [\"A closeup of someone's fingers as they use a keyboard.\",\n",
       "  'A kid is typing on a laptop keyboard.',\n",
       "  \"A child's hands hitting keys on an old computer.\"]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading vocab from ~/obj/vocab.pkl\n",
      "Vocabulary successfully loaded\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [410, 500, 3] at entry 0 and [640, 546, 3] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d3f5af999365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moriginal_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtransformed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtransformed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [410, 500, 3] at entry 0 and [640, 546, 3] at entry 2"
     ]
    }
   ],
   "source": [
    "test_loader = load_data(test_image_ids.keys(), image_folder_path, mode = 'test')\n",
    "original_image, image = next(iter(test_loader))\n",
    "transformed_image = image.numpy()\n",
    "transformed_image = np.squeeze(transformed_image)\\\n",
    "                    .transpose((1, 2, 0))\n",
    "\n",
    "# Print sample image, before and after pre-processing\n",
    "plt.imshow(np.squeeze(original_image))\n",
    "plt.title('example image')\n",
    "plt.show()\n",
    "plt.imshow(transformed_image)\n",
    "plt.title('transformed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify values for embed_size and hidden_size - we use the same values as in training step\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "# Get the vocabulary and its size\n",
    "vocab = test_loader.dataset.vocab\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize the encoder and decoder, and set each to inference mode\n",
    "encoder = EncoderCNN(embed_size)\n",
    "encoder.eval()\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "decoder.eval()\n",
    "\n",
    "# Load the pre-trained weights\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "# Move models to GPU if CUDA is available.\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "features = encoder(image).unsqueeze(1)\n",
    "output = decoder.sample_beam_search(features)\n",
    "print('example output:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Evaluate performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n",
      "captions of 0 images are added\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_test_indices\n",
    "sample_size = 100\n",
    "test_indices = get_test_indices(training_image_ids, sample_size, mode = 'balanced_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
