{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Neutral Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preparing Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import get_activity_list, get_gender_nouns, get_qualified_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary activity_image_ids is saved as pickle in ~/obj/\n",
      "Dictionary gender_nouns_lookup is saved as pickle in ~/obj/\n",
      "\n",
      "Evaluating ground truth labels in train set\n",
      "\n",
      "Caption 0 processed, out of 414113 captions\n",
      "No. of qualified images processed: 0\n",
      "\n",
      "Caption 100000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 6452\n",
      "\n",
      "Caption 200000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 13359\n",
      "\n",
      "Caption 300000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 24080\n",
      "\n",
      "Caption 400000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 34712\n",
      "\n",
      "Evaluating ground truth labels in val set\n",
      "\n",
      "Caption 0 processed, out of 202654 captions\n",
      "No. of qualified images processed: 35500\n",
      "\n",
      "Caption 100000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 42016\n",
      "\n",
      "Caption 200000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 52292\n",
      "List qualified_image_ids is saved as csv in ~/data/list/\n",
      "Dictionary captions_dict is saved as pickle in ~/obj/\n",
      "Dictionary im_gender_csv is saved as pickle in ~/obj/\n"
     ]
    }
   ],
   "source": [
    "annotations_path = './data/annotations/'\n",
    "get_activity_list(save_file = True)\n",
    "get_gender_nouns(save_file = True)\n",
    "get_qualified_dataset(annotations_path, save_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Select Training Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our motivation of the project is to counter the bias in the dataset. As ground truth labels are not availabie from the original COCO dataset, we are experimenting with different methods of balancing the dataset. In the **get_training_data** function in data_utils.py, there are 8 different modes of generating data.\n",
    "\n",
    "    - random: randomized selection of qualified images\n",
    "    - balanced_mode: balanced ratio between male, female and neutral\n",
    "    - balanced_clean: balanced ratio between male, female and neutral, only use images when all captions agree on using the same gender\n",
    "    - balanced_gender_only: same as balanced_mode, but without neutral captions\n",
    "    - balanced_clean_noun: balanced ratio between male, female and neutral, only use images when all captions agree on using the same noun\n",
    "    - clean_noun: only use images when all captions agree on the same noun\n",
    "    - activity_balanced: from activity tagged image sets, choose same ratio of male, female, neutral image\n",
    "    - activity_balanced_clean: similar to activity_balanced, but all captions must agree on the same gender\n",
    "    \n",
    "Note that it is possible that output size may be smaller than training_size, especially for activity_balanced and activity_balanced_clean. As for certain activities, the sample size of clean data might be limited for some classes, e.g. women wearing tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions of 1000 images are added\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_training_indices, train_test_split\n",
    "\n",
    "sample_size = 1000\n",
    "test_size = 0.3\n",
    "training_image_ids, training_captions_dict = get_training_indices(sample_size = sample_size, mode = \"balanced_clean\")\n",
    "train_image_ids, test_image_ids, gender_train, gender_test = train_test_split(training_image_ids, test_size = test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading activity_image_ids from ~/obj/activity_image_ids.pkl\n",
      "Tokenize captions: (0, 372)\n",
      "Tokenize captions: (100, 372)\n",
      "Tokenize captions: (200, 372)\n",
      "Tokenize captions: (300, 372)\n",
      "vocab saved  as ~/obj/vocab.pkl\n",
      "Vocabulary successfully created\n",
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading activity_image_ids from ~/obj/activity_image_ids.pkl\n",
      "Loading vocab from ~/obj/vocab.pkl\n",
      "Vocabulary successfully loaded\n"
     ]
    }
   ],
   "source": [
    "from model_utils import load_data\n",
    "\n",
    "image_folder_path = './data/images/'\n",
    "\n",
    "train_loader = load_data(image_folder_path, mode = 'train', sample_size = 1000)\n",
    "val_loader = load_data(image_folder_path, mode = 'val', sample_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/images/val2014/COCO_val2014_000000265550.jpg',\n",
       " './data/images/val2014/COCO_val2014_000000106073.jpg',\n",
       " './data/images/val2014/COCO_val2014_000000109340.jpg',\n",
       " './data/images/val2014/COCO_val2014_000000150235.jpg',\n",
       " './data/images/val2014/COCO_val2014_000000099747.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "a = glob.glob(\"./data/images/val2014/*\")\n",
    "a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "# Sample a subset of captions with a randomized length\n",
    "indices = train_loader.dataset.get_indices()\n",
    "\n",
    "# Create and assign batch sampler to retrieve a batch with the sampled indices\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "train_loader.batch_sampler.sampler = new_sampler\n",
    "    \n",
    "# Load one batch\n",
    "# images, captions = next(iter(data_loader))\n",
    "\n",
    "# Obtain the batch\n",
    "for batch in train_loader:\n",
    "    images, captions = batch[0], batch [1]\n",
    "    \n",
    "print('images.shape:', images.shape)\n",
    "print('captions.shape:', captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps: 38\n",
      "Number of training steps: 38\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 10\n",
    "vocab_size = len(train_loader.dataset.vocab)\n",
    "\n",
    "# Initialize CNN and RNN\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss().cuda()\\\n",
    "if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) + list(encoder.bn.parameters())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(params=params, lr=0.001)\n",
    "\n",
    "# Calculate total number of training steps per epoch\n",
    "total_train_step = math.ceil(len(train_loader.dataset.captions_len) / train_loader.batch_sampler.batch_size)\n",
    "print (\"Number of training steps:\", total_train_step)\n",
    "total_val_step = math.ceil(len(val_loader.dataset.captions_len) / val_loader.batch_sampler.batch_size)\n",
    "print (\"Number of training steps:\", total_val_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val step [38/38], 57s, Loss: 2.9014, Perplexity: 18.1996, Bleu-4: 0.0479Validation Bleu-4 improved from -inf to 0.0635, saving model to best-model.pkl\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75278943d3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                format(best_val_bleu, val_bleu))\n\u001b[1;32m     20\u001b[0m         \u001b[0mbest_val_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_bleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best-model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n\u001b[1;32m     23\u001b[0m                    val_bleu, val_bleus, epoch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from model_utils import train, validate\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_bleus = []\n",
    "best_val_bleu = float(\"-INF\")\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(train_loader, encoder, decoder, criterion, optimizer, \n",
    "                       vocab_size, epoch, total_train_step)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss, val_bleu = validate(val_loader, encoder, decoder, criterion,\n",
    "                                  train_loader.dataset.vocab, epoch, total_val_step)\n",
    "    val_losses.append(val_loss)\n",
    "    val_bleus.append(val_bleu)\n",
    "    if val_bleu > best_val_bleu:\n",
    "        print (\"Validation Bleu-4 improved from {:0.4f} to {:0.4f}, saving model to best-model.pkl\".\n",
    "               format(best_val_bleu, val_bleu))\n",
    "        best_val_bleu = val_bleu\n",
    "        filename = os.path.join(\"./models\", \"best-model.pkl\")\n",
    "        save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "                   val_bleu, val_bleus, epoch)\n",
    "    else:\n",
    "        print (\"Validation Bleu-4 did not improve, saving model to model-{}.pkl\".format(epoch))\n",
    "    # Save the entire model anyway, regardless of being the best model so far or not\n",
    "    filename = os.path.join(\"./models\", \"model-{}.pkl\".format(epoch))\n",
    "    save_epoch(filename, encoder, decoder, optimizer, train_losses, val_losses, \n",
    "               val_bleu, val_bleus, epoch)\n",
    "    print (\"Epoch [%d/%d] took %ds\" % (epoch, num_epochs, time.time() - start_time))\n",
    "    if epoch > 5:\n",
    "        # Stop if the validation Bleu doesn't improve for 3 epochs\n",
    "        if early_stopping(val_bleus, 3):\n",
    "            break\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Train Model/ Load Model Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_ids, test_image_ids, gender_train, gender_test = train_test_split(training_image_ids, im_gender_summary, test_size = 0.3)\n",
    "\n",
    "                                                    \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model weights from XXX to ./model/ of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Predict on selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
