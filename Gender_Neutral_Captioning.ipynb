{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Neutral Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Preparing Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_image_ids saved  as ~/obj/activity_image_ids.pkl\n",
      "gender_nouns_lookup saved  as ~/obj/gender_nouns_lookup.pkl\n",
      "Loading gender_nouns_lookup from ~/obj/gender_nouns_lookup.pkl\n",
      "\n",
      "Evaluating ground truth labels in train set\n",
      "Caption 0 processed, out of 414113 captions\n",
      "No. of qualified images processed: 0\n",
      "Caption 100000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 6452\n",
      "Caption 200000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 13359\n",
      "Caption 300000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 24080\n",
      "Caption 400000 processed, out of 414113 captions\n",
      "No. of qualified images processed: 34712\n",
      "\n",
      "Evaluating ground truth labels in val set\n",
      "Caption 0 processed, out of 202654 captions\n",
      "No. of qualified images processed: 35500\n",
      "Caption 100000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 42016\n",
      "Caption 200000 processed, out of 202654 captions\n",
      "No. of qualified images processed: 52292\n",
      "saved in ./data/list/qualified_image_ids.csv\n",
      "captions_dict saved  as ~/obj/captions_dict.pkl\n",
      "im_gender_summary saved  as ~/obj/im_gender_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_activity_list, get_gender_nouns, get_qualified_dataset\n",
    "\n",
    "annotations_path = './data/annotations/'\n",
    "get_activity_list(save_file = True)\n",
    "get_gender_nouns(save_file = True)\n",
    "get_qualified_dataset(annotations_path, save_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select method to generate training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our motivation of the project is to counter the bias in the dataset. As ground truth labels are not availabie from the original COCO dataset, we are experimenting with different methods of balancing the dataset. In the **get_training_indices** function in data_utils.py, there are 8 different modes of generating data.\n",
    "\n",
    "- random: randomized selection of qualified images\n",
    "- balanced_mode: balanced ratio between male, female and neutral\n",
    "- balanced_clean: balanced ratio between male, female and neutral, only use images when all captions agree on using the same gender\n",
    "- balanced_gender_only: same as balanced_mode, but without neutral captions\n",
    "- balanced_clean_noun: balanced ratio between male, female and neutral, only use images when all captions agree on using the same noun\n",
    "- activity_balanced: from activity tagged image sets, choose same ratio of male, female, neutral image\n",
    "- activity_balanced_clean: similar to activity_balanced, but all captions must agree on the same gender\n",
    "    \n",
    "Note that it is possible that output size may be smaller than training_size, especially for activity_balanced and activity_balanced_clean. As for certain activities, the sample size of clean data might be limited for some classes, e.g. women wearing tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading activity_image_ids from ~/obj/activity_image_ids.pkl\n",
      "captions of 100 images are added\n",
      "captions of 200 images are added\n",
      "captions of 300 images are added\n",
      "training_image_ids saved  as ~/obj/training_image_ids.pkl\n",
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_training_indices, train_test_split\n",
    "\n",
    "sample_size = 500\n",
    "test_size = 0.3\n",
    "training_image_ids, training_captions_dict = get_training_indices(sample_size = sample_size, mode = \"activity_balanced\")\n",
    "train_image_ids, val_image_ids, gender_train, gender_val = train_test_split(training_image_ids, test_size = test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to train model. In function **train_model** in model_utils, there are two modes avaialble:\n",
    "- reg: regular training loss function\n",
    "- gender_neural: alternative loss function that penalizes gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Tokenize captions: (0, 839)\n",
      "vocab saved  as ~/obj/vocab.pkl\n",
      "Vocabulary successfully created\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading vocab from ~/obj/vocab.pkl\n",
      "Vocabulary successfully loaded\n",
      "\n",
      "\n",
      "Loaders successfully set up . . .\n",
      "\n",
      "\n",
      "Checking shape of sample batch . . .\n",
      "images.shape: torch.Size([10, 3, 224, 224])\n",
      "captions.shape: torch.Size([10, 14])\n",
      "\n",
      "\n",
      "Initialize model . . .\n",
      "\n",
      "\n",
      "Calculate total number of steps per epoch . . .\n",
      "Number of training steps: 84\n",
      "Number of training steps: 37\n",
      "\n",
      "\n",
      "Training model . . .\n",
      "Epoch 1, Val step [37/37], 9s, Loss: 3.7706, Perplexity: 43.4055, Bleu-4: 0.0348Validation Bleu-4 improved from -inf to 0.0460, saving model to best-model.pkl\n",
      "Epoch [1/10] took 33s\n",
      "Epoch 2, Val step [37/37], 9s, Loss: 3.2074, Perplexity: 24.7146, Bleu-4: 0.0476Validation Bleu-4 improved from 0.0460 to 0.0606, saving model to best-model.pkl\n",
      "Epoch [2/10] took 32s\n",
      "Epoch 3, Val step [37/37], 9s, Loss: 2.3301, Perplexity: 10.2790, Bleu-4: 0.0656Validation Bleu-4 improved from 0.0606 to 0.0685, saving model to best-model.pkl\n",
      "Epoch [3/10] took 32s\n",
      "Epoch 4, Val step [37/37], 9s, Loss: 2.4579, Perplexity: 11.6808, Bleu-4: 0.0498Validation Bleu-4 improved from 0.0685 to 0.0706, saving model to best-model.pkl\n",
      "Epoch [4/10] took 32s\n",
      "Epoch 5, Val step [37/37], 9s, Loss: 2.5709, Perplexity: 13.0773, Bleu-4: 0.0594Validation Bleu-4 did not improve, saving model to model-5.pkl\n",
      "Epoch [5/10] took 31s\n",
      "Epoch 6, Val step [37/37], 9s, Loss: 2.7779, Perplexity: 16.0845, Bleu-4: 0.0645Validation Bleu-4 improved from 0.0706 to 0.0789, saving model to best-model.pkl\n",
      "Epoch [6/10] took 32s\n",
      "Epoch 7, Val step [37/37], 9s, Loss: 2.4790, Perplexity: 11.9298, Bleu-4: 0.0890Validation Bleu-4 did not improve, saving model to model-7.pkl\n",
      "Epoch [7/10] took 31s\n",
      "Epoch 8, Val step [37/37], 9s, Loss: 2.2582, Perplexity: 9.5662, Bleu-4: 0.06600Validation Bleu-4 did not improve, saving model to model-8.pkl\n",
      "Epoch [8/10] took 31s\n",
      "Epoch 9, Val step [37/37], 9s, Loss: 2.2039, Perplexity: 9.0598, Bleu-4: 0.11292Validation Bleu-4 improved from 0.0789 to 0.1057, saving model to best-model.pkl\n",
      "Epoch [9/10] took 32s\n",
      "Epoch 10, Val step [37/37], 9s, Loss: 2.6160, Perplexity: 13.6807, Bleu-4: 0.0660Validation Bleu-4 did not improve, saving model to model-10.pkl\n",
      "Epoch [10/10] took 31s\n"
     ]
    }
   ],
   "source": [
    "from model_utils import train_model\n",
    "\n",
    "image_folder_path = './data/images/'\n",
    "batch_size = 32\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 10\n",
    "\n",
    "# train_model(train_image_ids, val_image_ids, image_folder_path, batch_size, embed_size, \\\n",
    "#             hidden_size, num_epochs, mode = 'reg')\n",
    "train_model(train_image_ids, val_image_ids, image_folder_path, batch_size, embed_size, \\\n",
    "            hidden_size, num_epochs, mode = 'gender_neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Predict on test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model weights from XXX to ./model/ of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is loaded from ./testmodels/balanced_clean_reg_5000/best-model.pkl . . .\n",
      "Loaded vocab file of pretrained model\n",
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Vocabulary successfully loaded\n",
      "(5, 3, 224, 224) torch.Size([5, 640, 444, 3])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-624ce73f43df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m predict_from_COCO(image_folder_path = image_folder_path, vocab_path = vocab_path, model_path = model_path,\\\n\u001b[0;32m---> 14\u001b[0;31m                        training_image_ids_path = training_image_ids_path)\n\u001b[0m",
      "\u001b[0;32m~/src/nlp/gender-neutral-captioning/model_utils.py\u001b[0m in \u001b[0;36mpredict_from_COCO\u001b[0;34m(image_folder_path, vocab_path, model_path, training_image_ids_path, embed_size, hidden_size, mode)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0mtransformed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0mtransformed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;31m# Print sample image, before and after pre-processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "from model_utils import predict_from_COCO\n",
    "%matplotlib inline\n",
    "\n",
    "# Specify paths if the model is downloaded, otherwise by default, the function retrieves the trained model\n",
    "image_folder_path = './data/images/'\n",
    "# vocab_path = ''\n",
    "# model_path = ''\n",
    "# training_image_ids_path = ''\n",
    "vocab_path = './testmodels/balanced_clean_reg_5000/vocab.pkl'\n",
    "model_path = './testmodels/balanced_clean_reg_5000/best-model.pkl'\n",
    "training_image_ids_path = './testmodels/balanced_clean_reg_5000/training_image_ids.pkl'\n",
    "\n",
    "predict_from_COCO(image_folder_path = image_folder_path, vocab_path = vocab_path, model_path = model_path,\\\n",
    "                       training_image_ids_path = training_image_ids_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Evaluate performance of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function **get_test_indices** in data_utils to extract \n",
    "- random\n",
    "- balanced_mode: balanced between male, female and neutral gender nouns, using estimated Ground Truth labels\n",
    "- balanced_clean: balanced between male, female and neutral gender nouns, only using images when all captions agree on the same gender noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Best model is loaded from ./testmodels/balanced_clean_reg_5000/best-model.pkl . . .\n",
      "Loaded vocab file of pretrained model\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n",
      "Vocabulary successfully loaded\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-96e6e36d0648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m test_pred_captions = predict_for_test_samples(image_folder_path = image_folder_path,\\\n\u001b[1;32m     15\u001b[0m                      \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                        training_image_ids_path = training_image_ids_path)\n\u001b[0m",
      "\u001b[0;32m~/src/nlp/gender-neutral-captioning/model_utils.py\u001b[0m in \u001b[0;36mpredict_for_test_samples\u001b[0;34m(sample_size, image_folder_path, vocab_path, model_path, training_image_ids_path, embed_size, hidden_size, mode)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_image_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mpred_captions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_from_COCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_folder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_image_ids_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_image_ids_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_print\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mtest_pred_captions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_captions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/nlp/gender-neutral-captioning/model_utils.py\u001b[0m in \u001b[0;36mpredict_from_COCO\u001b[0;34m(image_folder_path, vocab_path, model_path, training_image_ids_path, embed_size, hidden_size, image_id, mode, is_print)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_print\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/nlp/gender-neutral-captioning/model.py\u001b[0m in \u001b[0;36msample_beam_search\u001b[0;34m(self, inputs, states, max_len, beam_width)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mnext_idx_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0mnext_idx_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mlog_p\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtop_log_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from model_utils import predict_for_test_samples\n",
    "%matplotlib inline\n",
    "\n",
    "# Specify paths if the model is downloaded, otherwise by default, the function retrieves the trained model\n",
    "image_folder_path = './data/images/'\n",
    "# vocab_path = ''\n",
    "# model_path = ''\n",
    "# training_image_ids_path = ''\n",
    "vocab_path = './testmodels/balanced_clean_reg_5000/vocab.pkl'\n",
    "model_path = './testmodels/balanced_clean_reg_5000/best-model.pkl'\n",
    "training_image_ids_path = './testmodels/balanced_clean_reg_5000/training_image_ids.pkl'\n",
    "sample_size = 100\n",
    "\n",
    "test_pred_captions = predict_for_test_samples(image_folder_path = image_folder_path,\\\n",
    "                     sample_size = sample_size, vocab_path = vocab_path, model_path = model_path,\\\n",
    "                       training_image_ids_path = training_image_ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading im_gender_summary from ~/obj/im_gender_summary.pkl\n",
      "Loading captions_dict from ~/obj/captions_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_test_indices\n",
    "\n",
    "sample_size = 100\n",
    "test_indices = get_test_indices(sample_size, training_image_ids, mode = 'balanced_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./testmodels/balanced_clean_reg_5000/best-model.pkl',\n",
       " './testmodels/balanced_clean_reg_5000/training_image_ids.pkl',\n",
       " './testmodels/balanced_clean_reg_5000/vocab.pkl',\n",
       " './testmodels/random_gender_neutral_500/best-model.pkl',\n",
       " './testmodels/random_gender_neutral_500/training_image_ids.pkl',\n",
       " './testmodels/random_gender_neutral_500/vocab.pkl',\n",
       " './testmodels/balanced_clean_reg_500/best-model.pkl',\n",
       " './testmodels/balanced_clean_reg_500/training_image_ids.pkl',\n",
       " './testmodels/balanced_clean_reg_500/vocab.pkl',\n",
       " './testmodels/activity_balanced_reg_500/best-model.pkl',\n",
       " './testmodels/activity_balanced_reg_500/training_image_ids.pkl',\n",
       " './testmodels/activity_balanced_reg_500/vocab.pkl',\n",
       " './testmodels/activity_balanced_gender_neutral_500/best-model.pkl',\n",
       " './testmodels/activity_balanced_gender_neutral_500/training_image_ids.pkl',\n",
       " './testmodels/activity_balanced_gender_neutral_500/vocab.pkl',\n",
       " './testmodels/balanced_clean_noun_reg_500/best-model.pkl',\n",
       " './testmodels/balanced_clean_noun_reg_500/training_image_ids.pkl',\n",
       " './testmodels/balanced_clean_noun_reg_500/vocab.pkl',\n",
       " './testmodels/balanced_mode_gender_neutral_500/best-model.pkl',\n",
       " './testmodels/balanced_mode_gender_neutral_500/training_image_ids.pkl',\n",
       " './testmodels/balanced_mode_gender_neutral_500/vocab.pkl',\n",
       " './testmodels/random_reg_500/best-model.pkl',\n",
       " './testmodels/random_reg_500/training_image_ids.pkl',\n",
       " './testmodels/random_reg_500/vocab.pkl',\n",
       " './testmodels/gender_only_reg_500/best-model.pkl',\n",
       " './testmodels/gender_only_reg_500/training_image_ids.pkl',\n",
       " './testmodels/gender_only_reg_500/vocab.pkl',\n",
       " './testmodels/balanced_mode_reg_500/best-model.pkl',\n",
       " './testmodels/balanced_mode_reg_500/training_image_ids.pkl',\n",
       " './testmodels/balanced_mode_reg_500/vocab.pkl',\n",
       " './testmodels/balanced_clean_noun_gender_neutral_500/best-model.pkl',\n",
       " './testmodels/balanced_clean_noun_gender_neutral_500/training_image_ids.pkl',\n",
       " './testmodels/balanced_clean_noun_gender_neutral_500/vocab.pkl',\n",
       " './testmodels/balanced_gender_only_gender_neutral_500/best-model.pkl',\n",
       " './testmodels/balanced_gender_only_gender_neutral_500/training_image_ids.pkl',\n",
       " './testmodels/balanced_gender_only_gender_neutral_500/vocab.pkl',\n",
       " './testmodels/balanced_clean_gender_neutral_500/best-model.pkl',\n",
       " './testmodels/balanced_clean_gender_neutral_500/training_image_ids.pkl',\n",
       " './testmodels/balanced_clean_gender_neutral_500/vocab.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob('./testmodels/*/*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = './testmodels/balanced_mode_reg/best-model.pkl'\n",
    "#vocab_path = './testmodels/balanced_mode_reg/vocab.pkl'\n",
    "#predict_and_show_image(image_folder_path = image_folder_path, vocab_path = vocab_path, model_path = model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
