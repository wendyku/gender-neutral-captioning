# Correcting Gender Bias in Captioning Model

## Motivation and Goal
It is a known danger that biases exist in machine learning and artificial intelligence models. These biases can be a result of an unbalanced dataset, human annotator biases embedded in the data labels (Zhao et al., 2017) or a problematic model. It is important to recognize and make effort to counter these gender biases, such that the model will not reinforce unfair societal norms.
Our project discusses and tackles gender bias in captioning models, so that the image captions will be neutral of gender stereotypes. Specifically, this project 1) identifies types of gender bias in the captioning model, 2) experiments with different methods in reducing gender bias and 3) interprets the success of the final model. We also aim at building an inclusive captioning model that can distinguish not only the gender binary (ie. man or woman) but also a third category (ie. person) based on visual appearance.

